{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5/isdIbuQwYgcqEaZKHNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andyvzcode/langchain-agents/blob/main/AGENTES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qs88fZP6PD_",
        "outputId": "542dfb88-8906-46f7-b580-7f65204af59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.21)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (3.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.2.2)\n",
            "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, langchain-huggingface\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "Successfully installed langchain-huggingface-0.1.2 tokenizers-0.21.0 transformers-4.47.0\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade transformers langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HugginFace ü§ó"
      ],
      "metadata": {
        "id": "m5OmH8i57ZYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 500},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyKxcitW65SE",
        "outputId": "fbff1bfe-ac21-441c-82a9-7e2750c9d319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is electroencephalography?\"\n",
        "hf.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "MLJOpPYB7Lg9",
        "outputId": "b5d8cdd2-2490-4294-8ea6-0d7e96d8c484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What is electroencephalography?\\n\\nelectroencephalography is the most reliable and sensitive medical diagnostic instrument of our time ‚Äî it also gives a wide range of diagnostic possibilities in all conditions and types of injuries. Electroencephalography allows us to see patients' brains, see if they have any specific physical or physiological signs related to cancer or heart disease or trauma. It is the most advanced and portable diagnostic instrument in the world. More information:\\n\\nWhat types of electroencephalography are used in diagnosing cancer (Treatment and prevention, Cancer Treatment and Prevention, Cancer Treatment, Radiation Therapy, Physiological Therapy and Other Medical Devices)?\\n\\nElectroencephalography helps determine when to call for medical help, when to come to a physical examination and for emergency medical care\\n\\nWhen to call for emergency medical services (hospital help)\\n\\nWhat does electroencephalography look like inside a body (e.g. on a bicycle)?\\n\\nIs there a good treatment to reduce or even eliminate the symptoms and signs of osteoporosis?\\n\\nThe symptoms of osteoporosis are often different for different types of cancers\\n\\nDoes electroencephalography have a higher risk of side effects\\n\\nIs electroencephalography a low-cost diagnostic tool?\\n\\nA good amount of research in this field has shown that certain types of cancer and their potential side effects are worse with electroencephalography than with other medical diagnostic instruments such as ultrasound and computed tomography (CT).\\n\\nWhy use electroencephalography when more people need cancer treatment?\\n\\nMany people with cancer need medical treatment for problems including stroke, renal disease, chronic obstructive pulmonary disease (COPD), multiple sclerosis and some other conditions. Electroencephalography can also help us more precisely determine the type of cancer we are seeing.\\n\\nThere are a lot of things you need to know about cancer and its symptoms, including what the exact mechanism is and which side effects should be avoided.\\n\\nFor more information on this field, visit the Cancer Treatment.com website. Click here for a list of links.\\n\\nFor more information about medical devices and how to order them, click here.\\n\\nWhat are the benefits?\\n\\nThe long-term benefit will not only be its lower costs, but also its greater health benefits.\\n\\nPeople have already begun to be persuaded that more medications are not needed even as a preventive measure like medical interventions. The study was conducted by a\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GEMINI GOOGLE"
      ],
      "metadata": {
        "id": "P9o7E2yR9InG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GEMINI')\n"
      ],
      "metadata": {
        "id": "f7TTGsBu9PT7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYWH7EeWpghR",
        "outputId": "f2bf79da-a576-411b-b26e-249f236767e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    google_api_key=api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "Zuia-w8bpiYD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg\n",
        "\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE8PfddUpoZn",
        "outputId": "e7d44b3a-b22d-4375-cd87-038747cbf1be"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'adore programmer.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat models"
      ],
      "metadata": {
        "id": "FJeorerh_EHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('OPENAPI')"
      ],
      "metadata": {
        "id": "JjVwWW2IBUv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1178abd3-1d9b-45f9-8cf4-6490cb813867"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo',\n",
        "                 temperature=0,\n",
        "                 max_tokens = 50,\n",
        "                 openai_api_key=API_KEY,\n",
        "                 )"
      ],
      "metadata": {
        "id": "yBNXbErEFfab"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [(\n",
        "    'system',\n",
        "    'Eres un profesor de programacion en python. Ense√±a a programar'\n",
        "    ),\n",
        "    (\n",
        "    'human', 'Quiero instalar python en windows'\n",
        "    )\n",
        "           ]"
      ],
      "metadata": {
        "id": "GnPMBphKGDpo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = llm.invoke(messages)"
      ],
      "metadata": {
        "id": "2_gme_aIGFGq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4CUAgoKTGUPE",
        "outputId": "73ef09e6-484b-4895-c818-00abccb1156b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¬°Claro! Aqu√≠ te dejo los pasos para instalar Python en Windows:\\n\\n1. Ve al sitio web oficial de Python en https://www.python.org/downloads/\\n2. Haz clic en el bot√≥n de descarga que correspon'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Messages : AI Messages, Human Messages, System Messages"
      ],
      "metadata": {
        "id": "EZDcVdgjmGlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model='gpt-3.5-turbo',\n",
        "                 temperature=0,\n",
        "                 openai_api_key=API_KEY,\n",
        "                 )"
      ],
      "metadata": {
        "id": "t8MSOirhnAHz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "messages = [ SystemMessage(content='Eres un asistente √∫til.'),\n",
        "            HumanMessage(content='Me ayudas a organizar las tareas del dia?'),\n",
        "             AIMessage(content='Claro! Que tareas necesitas completar hoy?'),\n",
        "             HumanMessage(content='Tengo que enviar un correo importante, hacer ejercicio y estudiar para un examen'),\n",
        "             AIMessage(content='Aqu√≠ tienes tu lista de tareas: 1. Enviar correo. 2. Hacer ejercicio. 3. Estudiar para el examen')\n",
        "]\n"
      ],
      "metadata": {
        "id": "q2lPpxIHm97T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7n6DVRRnH0h",
        "outputId": "25acc6d5-1762-4569-aa3f-ccec222d9a40"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='¬øEn qu√© orden te gustar√≠a abordar estas tareas?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 106, 'total_tokens': 120, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-39454607-f73b-452a-bcc3-75aa49668aee-0' usage_metadata={'input_tokens': 106, 'output_tokens': 14, 'total_tokens': 120, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Mw9K5QnI_L",
        "outputId": "d9e6fa56-094d-4b27-881b-293e0fa395c1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬øEn qu√© orden te gustar√≠a abordar estas tareas?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "trim_messages(\n",
        "    messages,\n",
        "    max_tokens=100,\n",
        "    strategy='last',\n",
        "    token_counter=ChatOpenAI(model='gpt-4o',  openai_api_key=API_KEY),\n",
        "    include_system = True # si quieres que incluya el sistema\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhXPzgMUoYP3",
        "outputId": "0ae828a3-363a-43ea-be35-39891dda1af3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Eres un asistente √∫til.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Me ayudas a organizar las tareas del dia?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Claro! Que tareas necesitas completar hoy?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Tengo que enviar un correo importante, hacer ejercicio y estudiar para un examen', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Aqu√≠ tienes tu lista de tareas: 1. Enviar correo. 2. Hacer ejercicio. 3. Estudiar para el examen', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vKB-qKVxpdKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Promt Template  \n",
        "\n",
        "Un prompt template es una estructura que toma variables de entrada y las convierte en una instrucci√≥n clara para un modelo de lenguaje. Estas variables se pasan en un diccionario donde cada clave representa un valor que ser√° utilizado para completar el template. Hay diferentes tipos de templates"
      ],
      "metadata": {
        "id": "qnU5M5cnp65z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## String prompt template"
      ],
      "metadata": {
        "id": "DyhHl-LfqDdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model='gpt-3.5-turbo',\n",
        "                 temperature=0,\n",
        "                 openai_api_key=API_KEY,\n",
        "                 )"
      ],
      "metadata": {
        "id": "WFiSSfSZqHrV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template('Dime un chiste {topic}')\n",
        "\n",
        "print(prompt_template.invoke({'topic':'gatos'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNFZ8sRwqI9Y",
        "outputId": "a6745ae5-014d-4bae-e66d-d3ae3ef692db"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Dime un chiste gatos'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Eres un asistente √∫til'),\n",
        "    ('user', 'Dime un chiste {topic}')\n",
        "])\n",
        "print(prompt_template.invoke({'topic':'gatos'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9eERnkkquGD",
        "outputId": "31c44b93-7162-4b96-b7b4-fb8e78523554"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Eres un asistente √∫til', additional_kwargs={}, response_metadata={}), HumanMessage(content='Dime un chiste gatos', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Eres un asistente √∫til'),\n",
        "    MessagesPlaceholder('msgs')\n",
        "\n",
        "])\n",
        "\n",
        "print(prompt_template.invoke({'msgs': [HumanMessage(content='hola'),HumanMessage(content='adios')]}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KP_tcC7q1-h",
        "outputId": "56e3108c-bf6f-4283-9812-3be91b149674"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Eres un asistente √∫til', additional_kwargs={}, response_metadata={}), HumanMessage(content='hola', additional_kwargs={}, response_metadata={}), HumanMessage(content='adios', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot prompting\n",
        "\n",
        "Few-shot Prompting es una t√©cnica que permite a los modelos de lenguaje aprender a responder de manera m√°s precisa utilizando ejemplos concretos dentro del mismo prompt."
      ],
      "metadata": {
        "id": "F7-SA4i2wbR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4o',\n",
        "                 temperature=0,\n",
        "                 openai_api_key=API_KEY,\n",
        "                 )"
      ],
      "metadata": {
        "id": "Dz8aZAfxwg4S"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke('Cuanto es 2 ü¶ú 9').content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D1QBmFztwisl",
        "outputId": "7aae0945-71e0-4b7d-977e-b1f8ee286720"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Parece que has usado un emoji de loro en tu pregunta. Si est√°s preguntando por una operaci√≥n matem√°tica, como una multiplicaci√≥n, entonces \\\\(2 \\\\times 9 = 18\\\\). Si te refieres a otra cosa, por favor proporciona m√°s contexto.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [{'input': '2 ü¶ú 2', 'output': '4'},\n",
        "            {'input': '2 ü¶ú 3', 'output': '5'},\n",
        "             {'input': '2 ü¶ú 4', 'output': '6'}\n",
        "            ]"
      ],
      "metadata": {
        "id": "jXMBO5UQw_mA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = ChatPromptTemplate(\n",
        "    [('human', '{input}'),\n",
        "     ('ai', '{output}')]\n",
        ")"
      ],
      "metadata": {
        "id": "BISCmMNvxBm3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples\n",
        ")"
      ],
      "metadata": {
        "id": "o4okBU-exD4_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_prompt.invoke({}).to_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLS2RpOVxFzm",
        "outputId": "a56f6250-2da3-4567-f5c3-8e504b2df320"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method ChatPromptValue.to_messages of ChatPromptValue(messages=[HumanMessage(content='2 ü¶ú 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 ü¶ú 3', additional_kwargs={}, response_metadata={}), AIMessage(content='5', additional_kwargs={}, response_metadata={})])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_prompt = ChatPromptTemplate.from_messages(\n",
        "    [('system', 'Eres un mago de las matematicas.'),\n",
        "     few_shot_prompt,\n",
        "     ('human', '{input}')\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "S_EPIF8fyeG6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = main_prompt | model"
      ],
      "metadata": {
        "id": "gAhzKyacyj6z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'input': 'Cuanto es 2 ü¶ú 9'}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SdsbkOrlygHk",
        "outputId": "c9494130-e377-42d9-cda1-b78b3363de1f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'11'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL Langchain Lenguaje"
      ],
      "metadata": {
        "id": "Q3uj-EpOyd6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', \"Traduce lo siguiente al {language}: \"),\n",
        "    ('human', '{text}')\n",
        "])"
      ],
      "metadata": {
        "id": "SCTVrPZIz160"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt_template | model | parser"
      ],
      "metadata": {
        "id": "liALjxaMz5jU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({'language': 'Italian', 'text': 'hello'})"
      ],
      "metadata": {
        "id": "wQ2v03S7z8Ic"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iZcSO3Ez-gP",
        "outputId": "e8951d66-8dbc-4306-9f03-bb51e8539d3f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sei addestrato su dati fino a ottobre 2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio: Chat Memory"
      ],
      "metadata": {
        "id": "x6db2tTT1JkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4o',\n",
        "                 temperature=0,\n",
        "                 openai_api_key=API_KEY,\n",
        "                 )"
      ],
      "metadata": {
        "id": "DKMW9NGT1JXG"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat_history = []\n",
        "if not chat_history:\n",
        "  system_message = SystemMessage(content='Eres un asistente √∫til')\n",
        "  chat_history.append(system_message)"
      ],
      "metadata": {
        "id": "6LYaqo-74xTf"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input('Haz una pregunta: ')\n",
        "chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "\n",
        "response = model.invoke(chat_history).content\n",
        "chat_history.append(AIMessage(content=response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk2d3ZOs4zMM",
        "outputId": "2f5e72f8-9d42-470b-b1f9-5c5e455d0edd"
      },
      "execution_count": 80,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Haz una pregunta: o dime cuando es navidad \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in chat_history:\n",
        "  print(message)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5t1nKoO43FI",
        "outputId": "c2b520d4-d3ce-4723-f868-a2009d4e202a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Eres un asistente √∫til' additional_kwargs={} response_metadata={}\n",
            "content='que dia es hoy ?' additional_kwargs={} response_metadata={}\n",
            "content='Lo siento, no puedo proporcionar la fecha actual ya que mi capacidad para acceder a informaci√≥n en tiempo real est√° desactivada. Te recomendar√≠a que verifiques la fecha en un dispositivo o calendario cercano.' additional_kwargs={} response_metadata={}\n",
            "content='Bueno dime la hora ' additional_kwargs={} response_metadata={}\n",
            "content='Lo siento, no puedo proporcionar la hora actual. Te recomendar√≠a que verifiques la hora en un reloj o dispositivo cercano.' additional_kwargs={} response_metadata={}\n",
            "content='o dime cuando es navidad ' additional_kwargs={} response_metadata={}\n",
            "content='La Navidad se celebra el 25 de diciembre de cada a√±o.' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE3sPoLk47SM",
        "outputId": "8a7a31db-67a5-44c2-cd70-695f27ba2bc5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Eres un asistente √∫til', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='que dia es hoy ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Lo siento, no puedo proporcionar la fecha actual ya que mi capacidad para acceder a informaci√≥n en tiempo real est√° desactivada. Te recomendar√≠a que verifiques la fecha en un dispositivo o calendario cercano.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Bueno dime la hora ', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Lo siento, no puedo proporcionar la hora actual. Te recomendar√≠a que verifiques la hora en un reloj o dispositivo cercano.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='o dime cuando es navidad ', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='La Navidad se celebra el 25 de diciembre de cada a√±o.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}